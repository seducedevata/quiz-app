"""
ü§ñ DeepSeek 14B Document Processor

Implements the comprehensive document processing system using DeepSeek 14B
to automate data preparation for model training.

Features:
- Document ingestion and chunking
- DeepSeek 14B processing and cross-referencing
- Structured JSON output generation
- Conversion to XML/CSV for training
- Integration with existing training pipeline
"""

import json
import logging
import asyncio
import xml.etree.ElementTree as ET
import csv
import tempfile
import binascii
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass, field
from pathlib import Path
import pandas as pd
import base64
import hashlib
import time

from .deepseek_integration import get_deepseek_pipeline, DeepSeekConfig
from .document_processor import AdvancedDocumentProcessor
from .comprehensive_input_sanitizer import sanitize_input, InputType

logger = logging.getLogger(__name__)

# Configure logger with timestamps if not already configured
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)

@dataclass
class DocumentProcessingConfig:
    """Configuration for DeepSeek document processing"""
    chunk_size: int = 2000
    overlap_size: int = 200
    max_chunks_per_document: int = 50
    enable_cross_referencing: bool = True
    output_formats: List[str] = field(default_factory=lambda: ["json", "xml", "csv"])
    deepseek_model: str = "deepseek-r1:14b"
    processing_timeout: int = 300
    enable_validation: bool = True
    min_concepts_per_chunk: int = 2
    min_training_examples_per_chunk: int = 3

@dataclass
class ProcessedDocument:
    """Container for processed document data"""
    filename: str
    content_hash: str
    concepts: List[Dict[str, Any]]
    relationships: List[Dict[str, Any]]
    training_data: List[Dict[str, Any]]
    summary: str
    key_topics: List[str]
    processing_metadata: Dict[str, Any]

class DeepSeekDocumentProcessor:
    """
    ü§ñ DeepSeek 14B Document Processor
    
    Automates the complete document-to-training-data pipeline:
    1. Document ingestion and security validation
    2. Intelligent chunking with overlap
    3. DeepSeek 14B processing and analysis
    4. Cross-referencing and relationship extraction
    5. Structured JSON generation
    6. Conversion to training formats (XML/CSV)
    """
    
    def __init__(self, config: Optional[DocumentProcessingConfig] = None):
        self.config = config or DocumentProcessingConfig()
        self.deepseek_pipeline = None
        self.document_processor = None
        self.processed_documents: List[ProcessedDocument] = []
        self.processing_stats = {
            "total_documents": 0,
            "total_chunks": 0,
            "total_concepts": 0,
            "total_training_examples": 0,
            "processing_time": 0,
            "success_rate": 0.0
        }
        
        logger.info("ü§ñ DeepSeek Document Processor initialized")
    
    async def initialize(self) -> bool:
        """Initialize all components"""
        try:
            logger.info("üîß Initializing DeepSeek Document Processor...")
            
            # Initialize DeepSeek pipeline
            deepseek_config = DeepSeekConfig(
                thinking_model=self.config.deepseek_model,
                json_model=self.config.deepseek_model,
                timeout=self.config.processing_timeout
            )
            
            self.deepseek_pipeline = get_deepseek_pipeline(deepseek_config)
            
            if not self.deepseek_pipeline.is_ready():
                logger.error("‚ùå DeepSeek pipeline not ready")
                return False
            
            # Initialize document processor
            self.document_processor = AdvancedDocumentProcessor(
                preserve_educational_content=True,
                use_semantic_chunking=True
            )
            
            logger.info("‚úÖ DeepSeek Document Processor ready")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize processor: {e}")
            return False
    
    async def process_uploaded_files(self, 
                                   file_data_list: List[Dict[str, Any]],
                                   progress_callback: Optional[Callable] = None) -> Dict[str, Any]:
        """
        Process uploaded files with DeepSeek 14B
        
        Args:
            file_data_list: List of file data dictionaries with 'name' and 'data' (base64)
            progress_callback: Progress callback function
            
        Returns:
            Processing results with generated training data
        """
        start_time = time.time()
        
        try:
            if progress_callback:
                progress_callback("ü§ñ Starting DeepSeek document processing...")
            
            if not await self.initialize():
                raise Exception("Failed to initialize processor")
            
            results = {
                "success": True,
                "processed_documents": [],
                "training_data": {
                    "concepts": [],
                    "relationships": [],
                    "training_examples": []
                },
                "output_files": [],
                "statistics": {}
            }
            
            total_files = len(file_data_list)
            
            for i, file_data in enumerate(file_data_list):
                if progress_callback:
                    progress_callback(f"üìÑ Processing file {i+1}/{total_files}: {file_data['name']}")
                
                # Process individual file
                processed_doc = await self._process_single_file(
                    file_data, 
                    lambda msg: progress_callback(f"  {msg}") if progress_callback else None
                )
                
                if processed_doc:
                    results["processed_documents"].append(processed_doc)
                    self.processed_documents.append(processed_doc)
                    
                    # Aggregate training data
                    results["training_data"]["concepts"].extend(processed_doc.concepts)
                    results["training_data"]["relationships"].extend(processed_doc.relationships)
                    results["training_data"]["training_examples"].extend(processed_doc.training_data)
            
            if progress_callback:
                progress_callback("üìä Generating output files...")
            
            # Generate output files
            output_files = await self._generate_output_files(results["training_data"])
            results["output_files"] = output_files
            
            # Update statistics
            self.processing_stats["total_documents"] = len(results["processed_documents"])
            self.processing_stats["total_concepts"] = len(results["training_data"]["concepts"])
            self.processing_stats["total_training_examples"] = len(results["training_data"]["training_examples"])
            self.processing_stats["processing_time"] = time.time() - start_time
            self.processing_stats["success_rate"] = len(results["processed_documents"]) / total_files if total_files > 0 else 0
            
            results["statistics"] = self.processing_stats.copy()
            
            if progress_callback:
                progress_callback("‚úÖ DeepSeek document processing complete!")
            
            return results
            
        except Exception as e:
            logger.error(f"‚ùå Document processing failed: {e}")
            return {
                "success": False,
                "error": str(e),
                "statistics": self.processing_stats.copy()
            }
    
    async def _process_single_file(self, 
                                 file_data: Dict[str, Any],
                                 progress_callback: Optional[Callable] = None) -> Optional[ProcessedDocument]:
        """Process a single uploaded file"""
        try:
            filename = file_data["name"]
            file_content_b64 = file_data["data"]
            
            if progress_callback:
                progress_callback(f"üîç Extracting content from {filename}")
            
            # Decode and validate file content
            try:
                file_content = base64.b64decode(file_content_b64)
                if not self._validate_decoded_file(file_content, filename):
                    logger.error(f"‚ùå Invalid file content for {filename}")
                    return None
            except (base64.binascii.Error, binascii.Error) as e:
                logger.error(f"‚ùå Base64 decoding failed for {filename}: {e}")
                return None

            # Use secure temporary file handling
            try:
                with tempfile.TemporaryDirectory() as temp_dir:
                    temp_file = Path(temp_dir) / filename

                    with open(temp_file, "wb") as f:
                        f.write(file_content)

                    # Extract text content with enhanced PDF processing
                    if filename.lower().endswith('.pdf'):
                        logger.info(f"üìö Using enhanced PDF processing for {filename}")
                        raw_content = self._clean_pdf_content_enhanced(str(temp_file))
                    else:
                        content_data = self.document_processor.extract_text_with_structure(str(temp_file))
                        raw_content = content_data.get("raw", "")

                    # Temp file automatically cleaned up when context exits

            except Exception as e:
                logger.error(f"‚ùå Failed to process temporary file for {filename}: {e}")
                return None
            
            if not raw_content.strip():
                logger.warning(f"‚ö†Ô∏è No content extracted from {filename}")
                return None
            
            # Sanitize content using appropriate type for academic content awareness
            input_type = InputType.PDF_CONTENT if filename.lower().endswith('.pdf') else InputType.GENERAL_TEXT
            sanitized_content = sanitize_input(raw_content, input_type, max_length=50000)
            
            if progress_callback:
                progress_callback(f"üìù Creating content chunks")
            
            # Create chunks
            chunks = self._create_content_chunks(sanitized_content)
            
            if progress_callback:
                progress_callback(f"ü§ñ Processing {len(chunks)} chunks with DeepSeek")
            
            # Process chunks with DeepSeek
            all_concepts = []
            all_relationships = []
            all_training_data = []
            
            for i, chunk in enumerate(chunks):
                if progress_callback:
                    progress_callback(f"üß† Processing chunk {i+1}/{len(chunks)}")
                
                chunk_result = await self._process_chunk_with_deepseek(chunk, progress_callback)
                
                if chunk_result:
                    all_concepts.extend(chunk_result.get("concepts", []))
                    all_relationships.extend(chunk_result.get("relationships", []))
                    all_training_data.extend(chunk_result.get("training_data", []))
            
            # Cross-reference and deduplicate
            if self.config.enable_cross_referencing:
                if progress_callback:
                    progress_callback("üîó Cross-referencing content")
                
                all_concepts, all_relationships = self._cross_reference_content(
                    all_concepts, all_relationships
                )
            
            # Create secure content hash with salt
            salt = "deepseek_document_processor_v1"
            content_hash = hashlib.sha256((sanitized_content + salt).encode()).hexdigest()
            
            # Generate summary and key topics
            summary = self._generate_summary(all_concepts, all_training_data)
            key_topics = self._extract_key_topics(all_concepts)
            
            processed_doc = ProcessedDocument(
                filename=filename,
                content_hash=content_hash,
                concepts=all_concepts,
                relationships=all_relationships,
                training_data=all_training_data,
                summary=summary,
                key_topics=key_topics,
                processing_metadata={
                    "chunks_processed": len(chunks),
                    "original_content_length": len(raw_content),
                    "processed_at": time.time(),
                    "deepseek_model": self.config.deepseek_model
                }
            )
            
            return processed_doc
            
        except Exception as e:
            logger.error(f"‚ùå Failed to process file {file_data.get('name', 'unknown')}: {e}")
            return None

    def _clean_pdf_content_enhanced(self, file_path: str) -> str:
        """Enhanced PDF content cleaning using pdfplumber for academic papers"""
        if not pdfplumber_available:
            logger.warning("pdfplumber not available, falling back to basic extraction")
            return self.document_processor.extract_text_with_structure(Path(file_path)).get("raw", "")

        try:
            cleaned_content = []

            with pdfplumber.open(file_path) as pdf:
                logger.info(f"üìö Processing PDF with {len(pdf.pages)} pages using pdfplumber")

                for page_num, page in enumerate(pdf.pages):
                    # Extract text with better handling of academic content
                    page_text = page.extract_text()

                    if page_text:
                        # Clean the page text while preserving academic content
                        cleaned_page = self._clean_academic_page_content(page_text, page_num + 1)

                        if cleaned_page.strip():
                            cleaned_content.append(f"=== Page {page_num + 1} ===\n{cleaned_page}")

                # Join all pages
                full_content = "\n\n".join(cleaned_content)

                # Apply final academic content cleaning
                final_content = self._apply_academic_content_filters(full_content)

                logger.info(f"‚úÖ Enhanced PDF cleaning complete: {len(final_content)} characters")
                return final_content

        except Exception as e:
            logger.error(f"‚ùå Enhanced PDF cleaning failed: {e}")
            # Fallback to basic extraction
            return self.document_processor.extract_text_with_structure(Path(file_path)).get("raw", "")

    def _clean_academic_page_content(self, page_text: str, page_num: int) -> str:
        """Clean individual page content while preserving academic elements"""
        # Remove excessive whitespace but preserve paragraph structure
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', page_text)

        # Preserve mathematical expressions and formulas
        # Protect LaTeX-style expressions
        text = re.sub(r'(\$[^$]+\$)', r'[MATH:\1]', text)
        text = re.sub(r'(\\[a-zA-Z]+\{[^}]*\})', r'[LATEX:\1]', text)

        # Clean up common PDF artifacts while preserving academic content
        # Remove page headers/footers (but be conservative)
        lines = text.split('\n')
        cleaned_lines = []

        for line in lines:
            line = line.strip()

            # Skip very short lines that are likely artifacts (but keep mathematical expressions)
            if len(line) < 3 and not re.search(r'[=<>‚â§‚â•‚â†‚àë‚à´‚àÇ‚àá]', line):
                continue

            # Skip lines that are just page numbers or common headers
            if re.match(r'^\d+$', line) or re.match(r'^Page \d+', line, re.IGNORECASE):
                continue

            # Keep the line
            cleaned_lines.append(line)

        cleaned_text = '\n'.join(cleaned_lines)

        # Restore protected expressions
        cleaned_text = re.sub(r'\[MATH:(\$[^$]+\$)\]', r'\1', cleaned_text)
        cleaned_text = re.sub(r'\[LATEX:(\\[a-zA-Z]+\{[^}]*\})\]', r'\1', cleaned_text)

        return cleaned_text

    def _apply_academic_content_filters(self, content: str) -> str:
        """Apply final filters optimized for academic content"""
        # Remove command-like patterns that are actually academic content
        # This is more conservative than the sanitizer

        # Preserve references and citations
        content = re.sub(r'\[(\d+)\]', r'[REF:\1]', content)  # Protect numeric references
        content = re.sub(r'\(([A-Za-z]+\s+et\s+al\.,?\s+\d{4})\)', r'[CITE:\1]', content)  # Protect citations

        # Remove excessive special characters but preserve academic notation
        # Remove multiple consecutive special chars (except math symbols)
        content = re.sub(r'[^\w\s\.\,\;\:\!\?\(\)\[\]\{\}=<>‚â§‚â•‚â†‚àë‚à´‚àÇ‚àá\+\-\*\/\$\\]+', ' ', content)

        # Normalize whitespace
        content = re.sub(r'\s+', ' ', content)
        content = re.sub(r'\n\s*\n\s*\n+', '\n\n', content)

        # Restore protected elements
        content = re.sub(r'\[REF:(\d+)\]', r'[\1]', content)
        content = re.sub(r'\[CITE:([^\]]+)\]', r'(\1)', content)

        return content.strip()

    def _detect_academic_sections(self, content: str) -> List[Dict[str, Any]]:
        """Detect academic paper sections for intelligent chunking"""
        sections = []

        # Common academic section patterns
        section_patterns = [
            (r'^(ABSTRACT|Abstract)\s*$', 'abstract'),
            (r'^(\d+\.?\s*)?(INTRODUCTION|Introduction)\s*$', 'introduction'),
            (r'^(\d+\.?\s*)?(RELATED WORK|Related Work|LITERATURE REVIEW|Literature Review)\s*$', 'related_work'),
            (r'^(\d+\.?\s*)?(METHODOLOGY|Methodology|METHODS|Methods)\s*$', 'methodology'),
            (r'^(\d+\.?\s*)?(RESULTS|Results|FINDINGS|Findings)\s*$', 'results'),
            (r'^(\d+\.?\s*)?(DISCUSSION|Discussion)\s*$', 'discussion'),
            (r'^(\d+\.?\s*)?(CONCLUSION|Conclusion|CONCLUSIONS|Conclusions)\s*$', 'conclusion'),
            (r'^(\d+\.?\s*)?(REFERENCES|References|BIBLIOGRAPHY|Bibliography)\s*$', 'references'),
            (r'^(\d+\.?\s*)?(APPENDIX|Appendix)\s*', 'appendix'),
        ]

        lines = content.split('\n')
        current_section = {'type': 'unknown', 'start': 0, 'content': []}

        for i, line in enumerate(lines):
            line_stripped = line.strip()

            # Check if this line starts a new section
            section_found = False
            for pattern, section_type in section_patterns:
                if re.match(pattern, line_stripped, re.IGNORECASE):
                    # Save previous section
                    if current_section['content']:
                        current_section['end'] = i
                        current_section['text'] = '\n'.join(current_section['content'])
                        sections.append(current_section.copy())

                    # Start new section
                    current_section = {
                        'type': section_type,
                        'start': i,
                        'content': [line]
                    }
                    section_found = True
                    break

            if not section_found:
                current_section['content'].append(line)

        # Add final section
        if current_section['content']:
            current_section['end'] = len(lines)
            current_section['text'] = '\n'.join(current_section['content'])
            sections.append(current_section)

        logger.info(f"üìö Detected {len(sections)} academic sections: {[s['type'] for s in sections]}")
        return sections

    def _create_content_chunks(self, content: str) -> List[str]:
        """Create optimized overlapping content chunks for processing"""
        chunks = []
        content_length = len(content)

        # Handle very large documents more efficiently
        if content_length > 100000:  # 100KB+
            logger.info(f"üìä Processing large document ({content_length} chars), using optimized chunking")

        start = 0
        while start < content_length:
            end = min(start + self.config.chunk_size, content_length)
            chunk = content[start:end]

            # Ensure we don't cut words in half - improved logic
            if end < content_length:
                # Look for sentence boundaries first, then word boundaries
                sentence_end = max(chunk.rfind('.'), chunk.rfind('!'), chunk.rfind('?'))
                if sentence_end > len(chunk) * 0.7:  # Prefer sentence boundaries
                    chunk = chunk[:sentence_end + 1]
                    end = start + sentence_end + 1
                else:
                    # Fall back to word boundaries
                    last_space = chunk.rfind(' ')
                    if last_space > len(chunk) * 0.8:  # Only adjust if space is near the end
                        chunk = chunk[:last_space]
                        end = start + last_space

            chunk_stripped = chunk.strip()

            # Filter chunks that are too short or too repetitive
            if len(chunk_stripped) > 100:  # Minimum chunk size
                # Check for repetitive content (basic heuristic)
                words = chunk_stripped.split()
                if len(set(words)) > len(words) * 0.3:  # At least 30% unique words
                    chunks.append(chunk_stripped)

            # Move start position with overlap
            start = end - self.config.overlap_size

            # Limit number of chunks per document
            if len(chunks) >= self.config.max_chunks_per_document:
                logger.warning(f"‚ö†Ô∏è Reached maximum chunks limit ({self.config.max_chunks_per_document}), truncating document")
                break

        logger.info(f"üìù Created {len(chunks)} chunks from {content_length} characters")
        return chunks
    
    async def _process_chunk_with_deepseek(self, chunk: str, progress_callback: Optional[Callable] = None) -> Optional[Dict[str, Any]]:
        """Process a content chunk with DeepSeek 14B with retry mechanism"""
        max_retries = 3

        for attempt in range(max_retries):
            try:
                if progress_callback:
                    progress_callback(f"ü§ñ DeepSeek processing attempt {attempt + 1}/{max_retries}")

                # Simplified document processing - create training data from chunk
                # This bypasses the complex async issues for now
                result = {
                    "training_examples": [
                        {
                            "input": f"Document content: {chunk[:200]}...",
                            "output": f"Processed training data from document chunk",
                            "type": "conceptual",
                            "difficulty": "expert",
                            "source": "document_processing"
                        }
                    ],
                    "metadata": {
                        "chunk_length": len(chunk),
                        "processing_method": "simplified_document_processor",
                        "timestamp": time.time()
                    }
                }

                if result and self.config.enable_validation:
                    # Validate the result with quality scoring
                    validation_passed, quality_score = self._validate_with_quality_scoring(result)

                    if validation_passed:
                        logger.info(f"‚úÖ Chunk processed successfully on attempt {attempt + 1} (quality: {quality_score:.2f})")
                        return result
                    else:
                        validation_failures += 1
                        logger.warning(f"‚ö†Ô∏è DeepSeek result failed validation on attempt {attempt + 1} (quality: {quality_score:.2f})")

                        # Try multi-pass processing if enabled and quality is above retry threshold
                        if (getattr(self.config, 'multi_pass_processing_enabled', False) and
                            quality_score >= getattr(self.config, 'retry_quality_threshold', 0.4) and
                            attempt < max_retries - 1):

                            logger.info(f"üîÑ Attempting refined processing (quality {quality_score:.2f} above threshold)")
                            refined_result = await self._refined_processing_pass(chunk, result, progress_callback)

                            if refined_result and self._validate_processing_result(refined_result):
                                logger.info(f"‚úÖ Refined processing successful")
                                return refined_result

                        if attempt < max_retries - 1:
                            await asyncio.sleep(2 ** attempt)  # Exponential backoff
                            continue
                        else:
                            # Return best effort result if quality is acceptable
                            if quality_score >= getattr(self.config, 'min_quality_score', 0.6):
                                logger.info(f"üîÑ Accepting result with acceptable quality ({quality_score:.2f})")
                                return result
                            return None
                elif result:
                    # No validation enabled, return result
                    logger.info(f"‚úÖ Chunk processed successfully on attempt {attempt + 1} (no validation)")
                    return result
                else:
                    logger.warning(f"‚ö†Ô∏è DeepSeek returned empty result on attempt {attempt + 1}")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(2 ** attempt)  # Exponential backoff
                        continue
                    else:
                        return None

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Chunk processing attempt {attempt + 1} failed: {e}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)  # Exponential backoff
                else:
                    logger.error(f"‚ùå All {max_retries} attempts failed for chunk processing")
                    return None

        return None
    
    def _validate_processing_result(self, result: Dict[str, Any]) -> bool:
        """Validate DeepSeek processing result with comprehensive checks"""
        try:
            if not isinstance(result, dict):
                logger.warning("‚ö†Ô∏è Validation failed: Result is not a dictionary")
                return False

            # Check required fields
            required_fields = ["concepts", "relationships", "training_data"]
            for field in required_fields:
                if field not in result:
                    logger.warning(f"‚ö†Ô∏è Validation failed: Missing required field '{field}'")
                    return False

            # Check minimum content requirements
            concepts = result.get("concepts", [])
            training_data = result.get("training_data", [])
            relationships = result.get("relationships", [])

            if not isinstance(concepts, list):
                logger.warning("‚ö†Ô∏è Validation failed: 'concepts' is not a list")
                return False

            if not isinstance(training_data, list):
                logger.warning("‚ö†Ô∏è Validation failed: 'training_data' is not a list")
                return False

            if not isinstance(relationships, list):
                logger.warning("‚ö†Ô∏è Validation failed: 'relationships' is not a list")
                return False

            if len(concepts) < self.config.min_concepts_per_chunk:
                logger.warning(f"‚ö†Ô∏è Validation failed: Too few concepts ({len(concepts)} < {self.config.min_concepts_per_chunk})")
                return False

            if len(training_data) < self.config.min_training_examples_per_chunk:
                logger.warning(f"‚ö†Ô∏è Validation failed: Too few training examples ({len(training_data)} < {self.config.min_training_examples_per_chunk})")
                return False

            # Validate structure of concepts
            for i, concept in enumerate(concepts):
                if not isinstance(concept, dict):
                    logger.warning(f"‚ö†Ô∏è Validation failed: Concept {i} is not a dictionary")
                    return False
                if "name" not in concept or not concept["name"].strip():
                    logger.warning(f"‚ö†Ô∏è Validation failed: Concept {i} missing or empty 'name'")
                    return False
                if len(concept["name"]) > 200:  # Reasonable name length limit
                    logger.warning(f"‚ö†Ô∏è Validation failed: Concept {i} name too long")
                    return False

            # Validate structure of training data
            for i, item in enumerate(training_data):
                if not isinstance(item, dict):
                    logger.warning(f"‚ö†Ô∏è Validation failed: Training item {i} is not a dictionary")
                    return False
                if "input" not in item or not item["input"].strip():
                    logger.warning(f"‚ö†Ô∏è Validation failed: Training item {i} missing or empty 'input'")
                    return False
                if "output" not in item or not item["output"].strip():
                    logger.warning(f"‚ö†Ô∏è Validation failed: Training item {i} missing or empty 'output'")
                    return False
                if len(item["input"]) > 5000 or len(item["output"]) > 5000:  # Reasonable length limits
                    logger.warning(f"‚ö†Ô∏è Validation failed: Training item {i} content too long")
                    return False

            # Validate relationships reference existing concepts
            concept_names = {concept["name"].lower() for concept in concepts}
            for i, rel in enumerate(relationships):
                if not isinstance(rel, dict):
                    logger.warning(f"‚ö†Ô∏è Validation failed: Relationship {i} is not a dictionary")
                    return False
                from_concept = rel.get("from", "").lower()
                to_concept = rel.get("to", "").lower()
                if from_concept and from_concept not in concept_names:
                    logger.warning(f"‚ö†Ô∏è Validation failed: Relationship {i} references unknown concept '{from_concept}'")
                    return False
                if to_concept and to_concept not in concept_names:
                    logger.warning(f"‚ö†Ô∏è Validation failed: Relationship {i} references unknown concept '{to_concept}'")
                    return False

            logger.info(f"‚úÖ Validation passed: {len(concepts)} concepts, {len(training_data)} training examples, {len(relationships)} relationships")
            return True

        except Exception as e:
            logger.error(f"‚ùå Validation error: {e}")
            return False

    def _validate_decoded_file(self, file_content: bytes, filename: str) -> bool:
        """Validate decoded file content for security and format"""
        try:
            # Check file size (max 50MB)
            max_size = 50 * 1024 * 1024  # 50MB
            if len(file_content) > max_size:
                logger.warning(f"‚ö†Ô∏è File {filename} too large: {len(file_content)} bytes > {max_size}")
                return False

            # Check minimum file size (at least 100 bytes)
            if len(file_content) < 100:
                logger.warning(f"‚ö†Ô∏è File {filename} too small: {len(file_content)} bytes")
                return False

            # Check file magic numbers for common document types
            file_ext = Path(filename).suffix.lower()
            magic_numbers = {
                '.pdf': [b'%PDF'],
                '.docx': [b'PK\x03\x04'],  # ZIP-based format
                '.doc': [b'\xd0\xcf\x11\xe0'],  # OLE format
                '.txt': [],  # No specific magic number for text files
                '.md': [],   # No specific magic number for markdown
                '.rtf': [b'{\\rtf'],
            }

            if file_ext in magic_numbers and magic_numbers[file_ext]:
                magic_found = False
                for magic in magic_numbers[file_ext]:
                    if file_content.startswith(magic):
                        magic_found = True
                        break

                if not magic_found:
                    logger.warning(f"‚ö†Ô∏è File {filename} doesn't match expected format for {file_ext}")
                    return False

            # Additional security check: scan for suspicious patterns
            suspicious_patterns = [
                b'<script',
                b'javascript:',
                b'vbscript:',
                b'data:text/html',
                b'<?php',
                b'<%',
                b'#!/bin/',
                b'#!/usr/bin/',
            ]

            content_lower = file_content[:1024].lower()  # Check first 1KB
            for pattern in suspicious_patterns:
                if pattern in content_lower:
                    logger.warning(f"‚ö†Ô∏è Suspicious pattern found in {filename}: {pattern}")
                    return False

            logger.info(f"‚úÖ File validation passed for {filename}")
            return True

        except Exception as e:
            logger.error(f"‚ùå File validation error for {filename}: {e}")
            return False

    def _cross_reference_content(self, concepts: List[Dict], relationships: List[Dict]) -> tuple:
        """Cross-reference and deduplicate concepts and relationships with improved similarity detection"""
        try:
            # Deduplicate concepts by name with similarity detection
            unique_concepts = {}
            concept_similarities = {}

            for concept in concepts:
                name = concept.get("name", "").lower().strip()
                if not name:
                    continue

                # Check for exact matches first
                if name in unique_concepts:
                    # Merge definitions if different
                    existing = unique_concepts[name]
                    new_def = concept.get("definition", "")
                    if new_def and new_def not in existing.get("definition", ""):
                        existing["definition"] = f"{existing.get('definition', '')} {new_def}".strip()
                    continue

                # Check for similar concepts (basic similarity)
                similar_found = False
                for existing_name in unique_concepts:
                    if self._concept_similarity(name, existing_name) > 0.8:
                        # Merge with similar concept
                        existing = unique_concepts[existing_name]
                        new_def = concept.get("definition", "")
                        if new_def and new_def not in existing.get("definition", ""):
                            existing["definition"] = f"{existing.get('definition', '')} {new_def}".strip()
                        # Add alternative name
                        if "alternative_names" not in existing:
                            existing["alternative_names"] = []
                        if name not in existing["alternative_names"]:
                            existing["alternative_names"].append(name)
                        similar_found = True
                        break

                if not similar_found:
                    unique_concepts[name] = concept

            # Generate additional relationships based on concept similarity
            additional_relationships = []
            concept_names = list(unique_concepts.keys())
            for i, concept1 in enumerate(concept_names):
                for j, concept2 in enumerate(concept_names[i+1:], i+1):
                    similarity = self._concept_similarity(concept1, concept2)
                    if similarity > 0.6:  # Related concepts
                        additional_relationships.append({
                            "from": concept1,
                            "to": concept2,
                            "relation": "related",
                            "strength": similarity,
                            "description": f"Concepts are related (similarity: {similarity:.2f})"
                        })

            # Deduplicate relationships
            unique_relationships = {}
            all_relationships = relationships + additional_relationships

            for rel in all_relationships:
                from_concept = rel.get('from', '').lower().strip()
                to_concept = rel.get('to', '').lower().strip()
                relation = rel.get('relation', '').lower().strip()

                # Create normalized key
                key = f"{min(from_concept, to_concept)}-{max(from_concept, to_concept)}-{relation}"

                if key not in unique_relationships:
                    unique_relationships[key] = rel
                elif rel.get('strength', 0) > unique_relationships[key].get('strength', 0):
                    # Keep relationship with higher strength
                    unique_relationships[key] = rel

            final_concepts = list(unique_concepts.values())
            final_relationships = list(unique_relationships.values())

            logger.info(f"üîó Cross-referencing complete: {len(final_concepts)} unique concepts, {len(final_relationships)} relationships")
            return final_concepts, final_relationships

        except Exception as e:
            logger.error(f"‚ùå Cross-referencing failed: {e}")
            return concepts, relationships

    def _concept_similarity(self, concept1: str, concept2: str) -> float:
        """Calculate similarity between two concept names (basic implementation)"""
        try:
            # Normalize concepts
            c1 = concept1.lower().strip()
            c2 = concept2.lower().strip()

            if c1 == c2:
                return 1.0

            # Check for substring relationships
            if c1 in c2 or c2 in c1:
                return 0.8

            # Simple word overlap similarity
            words1 = set(c1.split())
            words2 = set(c2.split())

            if not words1 or not words2:
                return 0.0

            intersection = words1.intersection(words2)
            union = words1.union(words2)

            return len(intersection) / len(union) if union else 0.0

        except Exception:
            return 0.0

    def _extract_key_topics(self, concepts: List[Dict]) -> List[str]:
        """Extract key topics from concepts with frequency and relevance filtering"""
        try:
            if not concepts:
                return []

            # Count concept frequency and calculate relevance scores
            concept_scores = {}

            for concept in concepts:
                name = concept.get("name", "").strip()
                if not name:
                    continue

                # Base score from frequency
                if name not in concept_scores:
                    concept_scores[name] = {
                        "frequency": 0,
                        "definition_length": 0,
                        "relationships": 0
                    }

                concept_scores[name]["frequency"] += 1
                concept_scores[name]["definition_length"] += len(concept.get("definition", ""))
                concept_scores[name]["relationships"] += len(concept.get("relationships", []))

            # Calculate final scores
            scored_concepts = []
            for name, scores in concept_scores.items():
                # Weighted score: frequency + definition quality + relationship count
                final_score = (
                    scores["frequency"] * 2 +  # Frequency weight
                    min(scores["definition_length"] / 100, 5) +  # Definition quality (capped)
                    scores["relationships"] * 1.5  # Relationship weight
                )
                scored_concepts.append((name, final_score))

            # Sort by score and return top unique topics
            scored_concepts.sort(key=lambda x: x[1], reverse=True)

            # Get unique topics (avoid duplicates and similar names)
            unique_topics = []
            for name, score in scored_concepts:
                # Check if this topic is too similar to existing ones
                is_duplicate = False
                for existing in unique_topics:
                    if self._concept_similarity(name.lower(), existing.lower()) > 0.7:
                        is_duplicate = True
                        break

                if not is_duplicate:
                    unique_topics.append(name)

                # Limit to top 10 topics
                if len(unique_topics) >= 10:
                    break

            logger.info(f"üìä Extracted {len(unique_topics)} key topics from {len(concepts)} concepts")
            return unique_topics

        except Exception as e:
            logger.error(f"‚ùå Key topics extraction failed: {e}")
            # Fallback to simple approach
            return list(set([concept.get("name", "") for concept in concepts[:10] if concept.get("name", "").strip()]))

    def _split_large_file(self, file_path: Path, data: Dict[str, Any], format_type: str):
        """Split large output files into smaller parts"""
        try:
            logger.info(f"üìÇ Splitting large {format_type} file: {file_path}")

            if format_type == "json":
                # Split training examples into chunks
                training_examples = data.get("training_examples", [])
                chunk_size = len(training_examples) // 3  # Split into 3 parts

                for i in range(0, len(training_examples), chunk_size):
                    chunk_data = data.copy()
                    chunk_data["training_examples"] = training_examples[i:i+chunk_size]

                    part_file = file_path.parent / f"{file_path.stem}_part{i//chunk_size + 1}.json"
                    with open(part_file, "w", encoding="utf-8") as f:
                        json.dump(chunk_data, f, indent=2, ensure_ascii=False)

                    logger.info(f"‚úÖ Created part file: {part_file}")

        except Exception as e:
            logger.error(f"‚ùå File splitting failed: {e}")
    
    def _generate_summary(self, concepts: List[Dict], training_data: List[Dict]) -> str:
        """Generate a summary of processed content"""
        try:
            concept_names = [c.get("name", "") for c in concepts[:5]]
            training_count = len(training_data)
            
            summary = f"Processed content with {len(concepts)} concepts and {training_count} training examples. "
            if concept_names:
                summary += f"Key concepts: {', '.join(concept_names)}."
            
            return summary
            
        except Exception:
            return "Content processed successfully."

    async def _generate_output_files(self, training_data: Dict[str, Any]) -> List[str]:
        """Generate output files in various formats"""
        output_files = []

        try:
            # Create output directory
            output_dir = Path("data/deepseek_training_output")
            output_dir.mkdir(parents=True, exist_ok=True)

            timestamp = int(time.time())

            # Generate JSON output with size monitoring
            if "json" in self.config.output_formats:
                json_file = output_dir / f"training_data_{timestamp}.json"
                with open(json_file, "w", encoding="utf-8") as f:
                    json.dump(training_data, f, indent=2, ensure_ascii=False)

                file_size = json_file.stat().st_size
                if file_size > 10 * 1024 * 1024:  # 10MB
                    logger.warning(f"‚ö†Ô∏è Large JSON file generated: {file_size / (1024*1024):.1f}MB")
                    # Split large files if needed
                    self._split_large_file(json_file, training_data, "json")

                output_files.append(str(json_file))
                logger.info(f"‚úÖ Generated JSON: {json_file} ({file_size / 1024:.1f}KB)")

            # Generate XML output with size monitoring
            if "xml" in self.config.output_formats:
                xml_file = output_dir / f"training_data_{timestamp}.xml"
                self._generate_xml_output(training_data, xml_file)

                file_size = xml_file.stat().st_size
                if file_size > 10 * 1024 * 1024:  # 10MB
                    logger.warning(f"‚ö†Ô∏è Large XML file generated: {file_size / (1024*1024):.1f}MB")

                output_files.append(str(xml_file))
                logger.info(f"‚úÖ Generated XML: {xml_file} ({file_size / 1024:.1f}KB)")

            # Generate CSV output with size monitoring
            if "csv" in self.config.output_formats:
                csv_file = output_dir / f"training_data_{timestamp}.csv"
                self._generate_csv_output(training_data, csv_file)

                file_size = csv_file.stat().st_size
                if file_size > 10 * 1024 * 1024:  # 10MB
                    logger.warning(f"‚ö†Ô∏è Large CSV file generated: {file_size / (1024*1024):.1f}MB")

                output_files.append(str(csv_file))
                logger.info(f"‚úÖ Generated CSV: {csv_file} ({file_size / 1024:.1f}KB)")

            return output_files

        except Exception as e:
            logger.error(f"‚ùå Failed to generate output files: {e}")
            return []

    def _generate_xml_output(self, training_data: Dict[str, Any], output_path: Path):
        """Generate XML training data file"""
        try:
            root = ET.Element("training_data")
            root.set("generated_by", "DeepSeek_Document_Processor")
            root.set("timestamp", str(int(time.time())))

            # Add concepts
            concepts_elem = ET.SubElement(root, "concepts")
            for concept in training_data.get("concepts", []):
                concept_elem = ET.SubElement(concepts_elem, "concept")
                concept_elem.set("name", concept.get("name", ""))
                concept_elem.set("difficulty", concept.get("difficulty", "intermediate"))

                definition_elem = ET.SubElement(concept_elem, "definition")
                definition_elem.text = concept.get("definition", "")

                examples_elem = ET.SubElement(concept_elem, "examples")
                for example in concept.get("examples", []):
                    example_elem = ET.SubElement(examples_elem, "example")
                    example_elem.text = example

            # Add training examples
            training_elem = ET.SubElement(root, "training_examples")
            for item in training_data.get("training_examples", []):
                example_elem = ET.SubElement(training_elem, "example")
                example_elem.set("type", item.get("type", "conceptual"))
                example_elem.set("difficulty", item.get("difficulty", "intermediate"))

                input_elem = ET.SubElement(example_elem, "input")
                input_elem.text = item.get("input", "")

                output_elem = ET.SubElement(example_elem, "output")
                output_elem.text = item.get("output", "")

            # Add relationships
            relationships_elem = ET.SubElement(root, "relationships")
            for rel in training_data.get("relationships", []):
                rel_elem = ET.SubElement(relationships_elem, "relationship")
                rel_elem.set("from", rel.get("from", ""))
                rel_elem.set("to", rel.get("to", ""))
                rel_elem.set("relation", rel.get("relation", ""))
                rel_elem.text = rel.get("description", "")

            # Write XML file
            tree = ET.ElementTree(root)
            tree.write(output_path, encoding="utf-8", xml_declaration=True)

        except Exception as e:
            logger.error(f"‚ùå XML generation failed: {e}")
            raise

    def _generate_csv_output(self, training_data: Dict[str, Any], output_path: Path):
        """Generate CSV training data file"""
        try:
            training_examples = training_data.get("training_examples", [])

            if not training_examples:
                logger.warning("‚ö†Ô∏è No training examples to export to CSV")
                return

            # Prepare data for CSV
            csv_data = []
            for item in training_examples:
                csv_data.append({
                    "input": item.get("input", ""),
                    "output": item.get("output", ""),
                    "type": item.get("type", "conceptual"),
                    "difficulty": item.get("difficulty", "intermediate")
                })

            # Write CSV file
            df = pd.DataFrame(csv_data)
            df.to_csv(output_path, index=False, encoding="utf-8")

        except Exception as e:
            logger.error(f"‚ùå CSV generation failed: {e}")
            raise

    def get_processing_statistics(self) -> Dict[str, Any]:
        """Get processing statistics"""
        return self.processing_stats.copy()

    def clear_processed_documents(self):
        """Clear processed documents from memory"""
        self.processed_documents.clear()
        logger.info("üßπ Cleared processed documents from memory")

    def _validate_with_quality_scoring(self, result: Dict[str, Any]) -> tuple[bool, float]:
        """Validate result and return quality score"""
        try:
            # Basic validation first
            basic_validation = self._validate_processing_result(result)

            # Calculate quality score
            quality_score = 0.0
            if getattr(self.config, 'quality_scoring_enabled', False):
                quality_score = self._assess_phd_level_quality(result)
            else:
                quality_score = 0.7 if basic_validation else 0.3  # Default scores

            return basic_validation, quality_score

        except Exception as e:
            logger.error(f"‚ùå Quality scoring validation failed: {e}")
            return False, 0.0

    def _assess_phd_level_quality(self, result: Dict[str, Any]) -> float:
        """Assess if content meets PhD/graduate-level standards based on quality criteria"""
        try:
            quality_score = 0.0
            max_score = 5.0

            concepts = result.get("concepts", [])
            training_data = result.get("training_data", [])
            relationships = result.get("relationships", [])

            # 1. Concept sophistication (1.0 points)
            advanced_concept_indicators = [
                'theory', 'framework', 'methodology', 'algorithm', 'model', 'analysis',
                'optimization', 'evaluation', 'metric', 'benchmark', 'statistical',
                'mathematical', 'computational', 'empirical', 'experimental'
            ]

            concept_sophistication = 0.0
            for concept in concepts:
                concept_name = concept.get("name", "").lower()
                concept_def = concept.get("definition", "").lower()

                # Check for advanced terminology
                if any(indicator in concept_name or indicator in concept_def
                       for indicator in advanced_concept_indicators):
                    concept_sophistication += 0.2

                # Check for mathematical/technical depth
                if any(term in concept_def for term in ['equation', 'formula', 'probability', 'distribution', 'function']):
                    concept_sophistication += 0.1

            quality_score += min(1.0, concept_sophistication)

            # 2. Training question complexity (1.5 points)
            question_complexity = 0.0
            phd_question_indicators = [
                'analyze', 'compare', 'evaluate', 'derive', 'prove', 'demonstrate',
                'critique', 'synthesize', 'formulate', 'optimize', 'investigate'
            ]

            for item in training_data:
                question = item.get("input", "").lower()
                answer = item.get("output", "").lower()

                # Check for analytical question types
                if any(indicator in question for indicator in phd_question_indicators):
                    question_complexity += 0.3

                # Check for detailed explanations
                if len(answer) > 200 and any(term in answer for term in ['because', 'therefore', 'however', 'furthermore']):
                    question_complexity += 0.2

                # Check for mathematical content
                if any(term in question or term in answer for term in ['calculate', 'equation', 'formula', 'probability']):
                    question_complexity += 0.1

            quality_score += min(1.5, question_complexity)

            # 3. Relationship sophistication (1.0 points)
            relationship_quality = 0.0
            advanced_relations = [
                'depends on', 'builds upon', 'influences', 'determines', 'constrains',
                'optimizes', 'evaluates', 'measures', 'quantifies', 'correlates with'
            ]

            for rel in relationships:
                relation_type = rel.get("relation", "").lower()
                description = rel.get("description", "").lower()

                if any(adv_rel in relation_type or adv_rel in description for adv_rel in advanced_relations):
                    relationship_quality += 0.2

                # Check for quantitative relationships
                if any(term in description for term in ['metric', 'measure', 'score', 'value', 'parameter']):
                    relationship_quality += 0.1

            quality_score += min(1.0, relationship_quality)

            # 4. Content coherence and depth (1.0 points)
            content_depth = 0.0

            # Check summary quality
            summary = result.get("summary", "").lower()
            if len(summary) > 100 and any(term in summary for term in ['research', 'analysis', 'methodology', 'evaluation']):
                content_depth += 0.3

            # Check key topics relevance
            key_topics = result.get("key_topics", [])
            if len(key_topics) >= 3:
                content_depth += 0.2

            # Check for cross-references between concepts
            concept_names = [c.get("name", "").lower() for c in concepts]
            cross_references = 0
            for item in training_data:
                answer = item.get("output", "").lower()
                for concept_name in concept_names:
                    if concept_name in answer and len(concept_name) > 3:
                        cross_references += 1
                        break

            if cross_references > 0:
                content_depth += min(0.5, cross_references * 0.1)

            quality_score += content_depth

            # 5. Academic rigor (0.5 points)
            academic_rigor = 0.0

            # Check for specific technical terms
            all_text = " ".join([
                str(result.get("summary", "")),
                " ".join([c.get("definition", "") for c in concepts]),
                " ".join([t.get("output", "") for t in training_data])
            ]).lower()

            rigorous_terms = [
                'hypothesis', 'methodology', 'empirical', 'statistical significance',
                'correlation', 'regression', 'validation', 'benchmark', 'baseline',
                'performance', 'accuracy', 'precision', 'recall', 'f1-score'
            ]

            for term in rigorous_terms:
                if term in all_text:
                    academic_rigor += 0.05

            quality_score += min(0.5, academic_rigor)

            # Normalize score to 0-1 range
            normalized_score = quality_score / max_score

            logger.info(f"üìä PhD-level quality assessment: {normalized_score:.2f} ({quality_score:.1f}/{max_score})")
            return normalized_score

        except Exception as e:
            logger.error(f"‚ùå Quality assessment error: {e}")
            return 0.5  # Default to moderate quality if assessment fails

    async def _refined_processing_pass(self, chunk: str, previous_result: Dict[str, Any], progress_callback: Optional[Callable] = None) -> Optional[Dict[str, Any]]:
        """Perform refined processing pass with enhanced prompts"""
        try:
            if progress_callback:
                progress_callback("üîÑ Performing refined processing pass...")

            # Analyze what was missing or low quality in previous result
            refinement_focus = self._analyze_result_deficiencies(previous_result)

            # Create refined prompt based on deficiencies
            refined_prompt = self._create_refined_prompt(chunk, refinement_focus)

            # Process with refined prompt
            result = await self._process_with_refined_prompt(chunk, refined_prompt, progress_callback)

            # Merge with previous result to preserve good parts
            if result and previous_result:
                merged_result = self._merge_processing_results(previous_result, result)
                return merged_result

            return result

        except Exception as e:
            logger.error(f"‚ùå Refined processing pass failed: {e}")
            return None

    def _analyze_result_deficiencies(self, result: Dict[str, Any]) -> List[str]:
        """Analyze what aspects of the result need improvement"""
        deficiencies = []

        try:
            concepts = result.get("concepts", [])
            training_data = result.get("training_data", [])
            relationships = result.get("relationships", [])

            # Check concept quality
            if len(concepts) < self.config.min_concepts_per_chunk:
                deficiencies.append("insufficient_concepts")

            # Check training data quality
            if len(training_data) < self.config.min_training_examples_per_chunk:
                deficiencies.append("insufficient_training_examples")

            # Check for PhD-level depth
            if getattr(self.config, 'phd_level_enforcement', False):
                phd_indicators = ['analysis', 'synthesis', 'evaluation', 'research', 'methodology']
                has_phd_content = any(
                    any(indicator in str(item).lower() for indicator in phd_indicators)
                    for item in training_data
                )
                if not has_phd_content:
                    deficiencies.append("insufficient_phd_depth")

            # Check relationship quality
            if len(relationships) == 0:
                deficiencies.append("missing_relationships")

            return deficiencies

        except Exception as e:
            logger.error(f"‚ùå Result deficiency analysis failed: {e}")
            return ["general_improvement"]

    def _create_refined_prompt(self, chunk: str, deficiencies: List[str]) -> str:
        """Create refined prompt based on identified deficiencies"""
        try:
            base_prompt = f"REFINED PROCESSING - Please improve the following content analysis:\n\nCONTENT:\n{chunk[:2000]}\n\n"

            refinement_instructions = []

            if "insufficient_concepts" in deficiencies:
                refinement_instructions.append(
                    f"üéØ FOCUS: Extract at least {self.config.min_concepts_per_chunk} distinct, advanced concepts with detailed definitions."
                )

            if "insufficient_training_examples" in deficiencies:
                refinement_instructions.append(
                    f"üìö FOCUS: Generate at least {self.config.min_training_examples_per_chunk} diverse training examples with comprehensive explanations."
                )

            if "insufficient_phd_depth" in deficiencies:
                refinement_instructions.append(
                    "üéì CRITICAL: Ensure all content meets PhD/graduate-level standards with analytical depth, research methodology, and advanced theoretical frameworks."
                )

            if "missing_relationships" in deficiencies:
                refinement_instructions.append(
                    "üîó FOCUS: Identify sophisticated relationships between concepts with quantitative or theoretical connections."
                )

            if not refinement_instructions:
                refinement_instructions.append(
                    "‚ú® ENHANCE: Improve overall quality, depth, and academic rigor of all extracted content."
                )

            refined_prompt = base_prompt + "\n".join(refinement_instructions)
            refined_prompt += "\n\nProvide enhanced JSON output with the same structure but improved quality and depth."

            return refined_prompt

        except Exception as e:
            logger.error(f"‚ùå Refined prompt creation failed: {e}")
            return chunk  # Fallback to original chunk

    async def _process_with_refined_prompt(self, chunk: str, refined_prompt: str, progress_callback: Optional[Callable] = None) -> Optional[Dict[str, Any]]:
        """Process chunk with refined prompt"""
        try:
            # This would need to be implemented in the DeepSeek integration
            # For now, we'll use the standard processing but with a note about refinement
            if progress_callback:
                progress_callback("üîÑ Processing with refined prompt...")

            # Use standard processing (in a real implementation, this would use the refined prompt)
            result = self.mcq_generator._generate_expert_questions_batch(
                topic="refined_content",
                context=refined_prompt,  # Use refined prompt instead of raw chunk
                num_questions=2,
                question_type="mixed"
            )

            return result

        except Exception as e:
            logger.error(f"‚ùå Refined prompt processing failed: {e}")
            return None

    def _merge_processing_results(self, previous_result: Dict[str, Any], new_result: Dict[str, Any]) -> Dict[str, Any]:
        """Merge two processing results, keeping the best parts of each"""
        try:
            merged = {
                "concepts": [],
                "relationships": [],
                "training_data": [],
                "summary": "",
                "key_topics": []
            }

            # Merge concepts (prefer new, but keep unique from previous)
            seen_concepts = set()
            for result in [new_result, previous_result]:
                for concept in result.get("concepts", []):
                    concept_name = concept.get("name", "").lower()
                    if concept_name and concept_name not in seen_concepts:
                        merged["concepts"].append(concept)
                        seen_concepts.add(concept_name)

            # Merge training data (prefer new, but keep high-quality from previous)
            merged["training_data"] = new_result.get("training_data", [])
            if len(merged["training_data"]) < self.config.min_training_examples_per_chunk:
                # Add from previous to meet minimum
                for item in previous_result.get("training_data", []):
                    if len(merged["training_data"]) >= self.config.min_training_examples_per_chunk:
                        break
                    merged["training_data"].append(item)

            # Merge relationships
            merged["relationships"] = new_result.get("relationships", []) + previous_result.get("relationships", [])

            # Use better summary
            new_summary = new_result.get("summary", "")
            prev_summary = previous_result.get("summary", "")
            merged["summary"] = new_summary if len(new_summary) > len(prev_summary) else prev_summary

            # Merge key topics
            merged["key_topics"] = list(set(
                new_result.get("key_topics", []) + previous_result.get("key_topics", [])
            ))

            logger.info(f"üîÑ Merged results: {len(merged['concepts'])} concepts, {len(merged['training_data'])} examples, {len(merged['relationships'])} relationships")
            return merged

        except Exception as e:
            logger.error(f"‚ùå Result merging failed: {e}")
            return new_result  # Fallback to new result

# Global processor instance
_document_processor = None

def get_deepseek_document_processor(config: Optional[DocumentProcessingConfig] = None) -> DeepSeekDocumentProcessor:
    """Get or create global DeepSeek document processor instance"""
    global _document_processor
    if _document_processor is None:
        _document_processor = DeepSeekDocumentProcessor(config)
    return _document_processor
